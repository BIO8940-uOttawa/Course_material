---
title: "Generalized linear mixed models with R"
subtitle: "GLMM"
author: "Julien Martin"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    css: ["assets/css/rutgers-bki.css", "assets/css/rutgers-fonts_og.css"]
    lib_dir: "assets/libs"
    self_contained: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options:
  chunk_output_type: console
---

# Shrinkage

- Defined as the fact that cluster values **shrink** toward the population mean in a mixed model.
- Best way to see it, compare **BLUPs** with prediction from regression by cluster approach

--

```{r baseline_gif, echo=F, eval=T, dev.args=list(bg = 'transparent'), cache=FALSE, fig.align = "center"}
knitr::include_graphics("assets/baseline.gif")
```

---

# Questions after reading *Bolker et al 2009*

- Difference between fixed and random effects

--

- When to transform data?
    - if you have a funky looking distribution of continuous data, is it always ok to transform to achieve normality if you donâ€™t violate any test assumptions?

--

- Walkthrough Figure 1 ?

--

- Get rid of non-significant fixed effects?
    - If important for my hypothesis, should I always keep them?
    - What if I have a fairly small dataset?

--

- How to choose a link function? Why not using the default?

--

- Can we go through example in Box 1?

---

# GLMM: What are they?

## GaCha Life Minie Movie

Video game allowing you to dress-up *anime* style characters

--

```{r glmm-anime, echo=F, eval=T, dev.args=list(bg = 'transparent'), cache=FALSE, fig.align = "center"}
knitr::include_graphics("assets/glmm-anime.png")
```
---

## Generalized linear mixed model

An extension to **Generalized linear model** and an extension to **linear mixed model**

GLMM expresses the transformed conditional expectation of the dependent variable y as a linear combination of the regression variables X

Model has 3 components

- a structural component or additive expression $\beta_0 + \beta_1 X_1 + ... + \beta_k X_k$
- a link function: $g(\mu)$
- a response distribution: **Gaussian**, **Binomial**, **Bernouilli**, **Poisson**, **negative binomial**, **zero-inflated ...**, **zero-truncated ...**, ...

<!-- $$
g(\mu_i) = \sum_{j=0}^p \beta_j X_{ij}
$$ -->

$$
g(\mu_i) = \beta_0 + \beta_1 X_1 + ... + \beta_k X_k
$$

and

$$
\mu_i = E(y_i | x_i) = g(\mu_i)^{-1}
$$

---

# How do you fit them?

In R:

- `glmer()` from `lme4` `r emo::ji("package")` same as `lmer()` but with a `family` argument
- `glmmPQL()` from `MASS` `r emo::ji("package")` (based on `lme()`)
- `glmmADMB()` from - `glmmADMB` `r emo::ji("package")` works well and flexible be beware
- `glmmTMB()` from `glmmTMB` `r emo::ji("package")` works well and flexible be beware
- `asreml()` from `glmmTMB` `r emo::ji("package")` great but not-free
- `MCMCglmm()` from `MCMCglmm` `r emo::ji("package")` great but Bayesian
- Choose you bayesian flavor `r emo::ji("package")`:
    - `stan`: `brms`, `rethinking`, `rstan`, ...
    - `BUGS`: `runjags`, `rjags`, ...

---

# Model assumptions

- Easy answer none or really few

- More advanced answer I am not sure, it is complicated

- Just check residuals I as usual

--

- Technically only 3 assumption:
    - **Variance is a function of the mean specific to the distribution used**
    - observations are independent
    - linear relation on the latent scale

### Generalized Linear Models do not care if the residual errors are normally distributed as long as the specified mean-variance relationship is satisfied by the data

---

# Choosing a link function

### A link function should map the stuctural component from $(-\infty,\infty)$ to the distribution interval (*e.g.* (0,1) for binomial)

So number of link function possible is extremley large.

--

### Choice of **link** function heavily influenced by field tradiditon

--
For binomial models

- **logit** assume modelling probability of an observation to be one
- **probit** assume binary outcome from a hidden gaussian variable (*i.e.* threshold model)
- **logit** & **probit** are really similar, both are symmetric but **probit** tapers faster. **logit** coefficient easier to interpret directly
- **cologlog** not-symmetrical

---

# Estimating repeatability ?

## Latent scale

## Observed scale ??????

- Using `rptR` `r emo::ji("package")` is the easiest or `QGGlmm` `r emo::ji("package")` (see associated citation for reference and explanations)

---

# Marginalized vs Conditioned estimates


Difference between **marginalized** and **conditioned** coefficients?

**GLMMadaptive** `r emo::ji("package")` is the only way I know to do easily get marginalized coefficients

---

# Practical / walkthrough Example box 1
